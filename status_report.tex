\documentclass[fleqn]{article}

%% Language and font encodings
\renewcommand{\familydefault}{\sfdefault} % use sans serif per default
\usepackage{helvet} % use helvetica per default

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=1.5cm,bottom=1.5cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath,amssymb} % for advanced math formulas
\setlength{\mathindent}{0pt}
\usepackage{hyperref}
\usepackage{graphicx}

\title{Status report - Siamese networks for instance retrieval}
\author{Matthias Kohl}

\begin{document}
\maketitle

This report serves as a status report for the internship on
Siamese networks for instance retrieval,
supervised by Georges QuÃ©not and Jean-Pierre Chevallet,
in collaboration with Maxime Portaz.

This report should not be seen as a scientific report.
Instead, it is a simple overview of the main ideas and
results of the internship so far and will be updated on the fly.

\section{Introduction}
We consider the problem of instance search for images.
The goal is as follows:
given a reference dataset of images and a query image,
retrieve the image from the reference dataset that
best suits the query image.

This problem is related to image classification:
if we consider each instance of an object as a class,
we can classify the query image and return any image
from that class in the reference dataset.

The difference with image classification is that
there is almost no variability within the classes,
as they are in fact a single object.
So the objective is to focus more on differentiating between objects,
rather than predicting the type of object.
Thus for a given query image, we try to tell how much it resembles each
image of the reference dataset.

\section{State of the art}
Previously, most systems were based on a bag-of-words approach
in order to match images~\cite{philbin_object_2007}.
Combined with better techniques of matching the bag-of-words descriptors,
this approach was previously the state-of-the-art~\cite{mikulik_learning_2013}.

Recently, the state-of-the-art has drastically improved due to the
addition of learned features, based on convolutional neural networks
(CNNs).
This new approach greatly improved
mean average precision scores~\cite{gordo_deep_2016} of the system
on the same datasets.

The general trend is to move from an approach based on a combination of
engineered features (bag-of-words combined with
support vector machines or SVMs) to a more end-to-end learning of
the matching of two images.

For this, the Siamese architecture is used, where the convolutional
features of two or more CNNs with shared weights are combined and a
multi-way loss is optimized, which discriminates between the features
of two or more images.
This concept was first introduced by
Chopra et al~\cite{chopra_learning_2005} to learn a dissimilarity metric
between two images.

It was then extended to a triplet loss by
Weinberger et al~\cite{weinberger_distance_2006}, which allows for
more stable convergence.

Using this triplet loss has achieved state-of-the-art results in both
face recognition~\cite{schroff_facenet:_2015} and
image retrieval~\cite{gordo_deep_2016}. This is mostly due to the usage
of far deeper CNNs, moving from architectures such as
AlexNet~\cite{krizhevsky_imagenet_2012} to VGG~\cite{simonyan_very_2014}
and finally Inception~\cite{szegedy_inception-v4_2016} and
ResNet~\cite{he_deep_2015}.

The higher depth of networks like ResNet and Inception as compared to
AlexNet allows for higher regularization and thus less over-fitting
to a specific dataset for these architectures. Batch normalization
increases this effect even more. This is a desirable trait for image
retrieval, as the variability within each instance is too small to
learn classification directly. So the better we can generalize
features learned from a bigger dataset to the reference dataset,
the better performance we can expect.

\section{Evaluation}
We are using the following datasets in our experiments:
\begin{enumerate}
    \item CLICIDE: dataset of photographs taken in an art museum,
    consisting exclusively of paintings. The dataset is characteristic
    because the different images for each instance consist of one
    global view of the instance and multiple sub-parts.

    Number of reference images: 3245
    Number of test images: 177
    Number of instances: 464
    \item GaRoFou\_I: dataset of photographs of an art museum,
    consisting of display cabinets, which contain sculptures,
    rocks and various types of objects.

    Number of reference images: 1068
    Number of test images: 184
    Number of instances: 311
    % \item GaRoFou_V: dataset of frames from a video taken by visitors
    % of the same museum as above.

    % Number of reference images: 2779
    % Number of test images: 2132
    % Number of instances: 215
    % \item Oxford5k: dataset of photographs of buildings taken in
    % the city of Oxford, UK.

    % Number of reference images: 5000 TODO
    % Number of test images: 55 TODO
    % Number of instances: 17 TODO
\end{enumerate}

\subsection{Evaluation metrics}
\subsubsection{Metrics definitions}
\paragraph{$Precision@k$}
For a test image and $m$ reference images and a ranking of the
reference images $Im_1, \dots, Im_m$, we define the number of relevant
images at $k$ with $k \leq m$:
$N^{rel}_k$ is the number of images in the sub-ranking
$Im_1, \dots, Im_k$ from the same instance than the test image.

We then define Precision at $k$ as:

\begin{equation}
Precision@k = \frac{N^{rel}_k}{k}
\end{equation}

\paragraph{MAP}
As above, for a test image, $m$ reference images and a ranking
of the reference images, we define the average precision:

\begin{equation}
AP = \frac{1}{N^{rel}_m} \sum_{k=1}^m Precision@k
\end{equation}

The MAP score is defined as the mean of the AP score over all test images.

\subsubsection{Used metrics}
As of now, we use the mean $Precision@1$ to evaluate the system.
It is the mean of the $Precision@1$ for all test images.
This is what we are most interested in: in the context of a
search system for retrieving art in a museum, we are only
interested in one result, as the user should not make a choice
out of several results.

To be comparable with other papers in the field, we will implement
the MAP score as well later on.

\section{Methodology}
The following approaches have been tested:

\subsection{Full classification features of a pre-trained CNN}
We use the features obtained from the last layer of a CNN
that was pre-trained on ImageNet. The feature dimension is thus
fixed at 1000, the number of classes in ImageNet.
\subsubsection{Motivation}
This is to set a baseline for other approaches and to reproduce results
from a previous paper (TODO cite CORIA paper).
\subsubsection{Specifics}
All images are resized to $227 \times 227$, using a nearest neighbor
re-sampling filter. This is to fit the default input dimensions of
a CNN used in ImageNet.

The following CNNs were tested:
\begin{enumerate}
    \item AlexNet
    \item ResNet-152
\end{enumerate}
\subsection{Convolutional features of a pre-trained CNN}
We use the features obtained from the last convolutional or pooling
layer of a CNN, pre-trained on ImageNet. The feature dimension depends
on the CNN and is added below in the specifics section.
\subsubsection{Motivation}
Convolutional features should perform better than full classification
features as we are not interested in capturing the class of objects in
the images, but only in capturing the various shapes and see if they
are similar in two images.
\subsubsection{Specifics}\label{sec:convspec}
All images are resized to $227 \times 227$, using a nearest neighbor
re-sampling filter.

The following CNNs were tested:
\begin{enumerate}
    \item AlexNet (feature dimension $6 \times 6 \times 256 = 9216$)
    \item ResNet-152 (feature dimension $8 \times 8 \times 2048 =131072$)
\end{enumerate}

\subsection{Convolutional features of a fine-tuned CNN}
We use the features obtained from the last convolutional or pooling layer
of a CNN, pre-trained on ImageNet, then fine-tuned
as a classification net on the reference dataset.

The feature dimensions are as described in Section~\ref{sec:convspec}.
\subsubsection{Motivation}
We choose convolutional features since they should perform better than
classification features. However, the high-level convolutions can be
fine-tuned to our dataset such that the high-level filters can capture
the shapes of that dataset better.
\subsubsection{Specifics}
All images are resized to $227 \times 227$, using a nearest neighbor
re-sampling filter.

For fine-tuning the CNN on classification, we use the following settings:
\begin{enumerate}
    \item Data augmentation: all images are shifted (in range $[-20\%, 20\%]$ in both directions), rotated (in range $[-45, 45]$ degrees), rescaled (with factor in range $[0.8, 1.2]$) and horizontally flipped at random.
    All parameters are chosen from a uniform distribution.

    The motivation behind the extensive data augmentation is that
    the training set is very small, so without data augmentation,
    large CNNs like AlexNet would over-fit the dataset.
    \item Learning rate: TODO
    \item TODO
\end{enumerate}

The following CNNs were tested:
\begin{enumerate}
    \item AlexNet (only last convolutional layer is fine-tuned)
    \item ResNet-152 (last 3 blocks are fine-tuned)
\end{enumerate}

\subsection{Features of a fine-tuned Siamese CNN using cosine similarity loss}
The general principle follows the architecture of
Gordo et al~\cite{gordo_deep_2016}, omitting the region pooling:
We use the convolutional features of a pre-trained CNN.
These features are $L2$-normalized, then a shifting layer and
fully connected layer are introduced to reduce the
feature dimension and the features are
$L2$-normalized again. This is similar to the architecture of
Schroff et al~\cite{schroff_facenet:_2015}, too.

Most importantly, we use a Siamese architecture along with a cosine
similarity loss. The loss is defined as follows for two images and
their feature vectors $x_1$ and $x_2$:

\begin{equation}
\mathcal{L} =
\begin{cases}
1 - cos(x_1, x_2) & \text{if the images are from the same instance}\\
\max(0, cos(x_1, x_2) - margin) & \text{if the images are from different instances}
\end{cases}
\end{equation}

$cos(x_1, x_2) = \frac{x_1 x_2}{\|x_1\|_2 \|x_2\|_2}$. For normalized
vectors, we have $cos(x_1, x_2) = x_1 x_2$, so the cosine similarity is
simply the dot product here.

\subsubsection{Motivation}
On small datasets, Siamese architectures should be able to obtain better
results than classification architectures as they are fine-tuned to
discriminate between two inputs, rather than simply output the class or
instance of an input.
Because of the missing variability for each instance, a classification
architecture will over-fit the dataset more than a Siamese architecture.

As described above, we omit the region pooling layer. There are two
reasons behind this: First, we only use images of the same size
as inputs, so we do not need to harmonize between different images.
Second, the images in our datasets are cleaner than images in the
datasets used by Gordo et al (Oxford5k, Paris6k, \dots), in the sense
that the objects are usually well centered in the images and fill out
most of the image. In other datasets, the objects searched for
only form a small part of the image. In this case, the rest of the image
is noise, making the network more difficult or impossible to train.

\subsubsection{Specifics}
All images are resized to $227 \times 227$, using a nearest neighbor
re-sampling filter. The margin in the loss is set to 0.

For fine-tuning the Siamese CNN, we use the following settings:
\begin{enumerate}
    \item Data augmentation: TODO
    \item Learning rate: TODO
    \item TODO
\end{enumerate}

The following choices of the pairs of images for training were tested:
\begin{enumerate}
    \item Choose all positive pairs and randomly choose negative pairs
    such that $90\%$ of the training set consists of negative pairs
    \item Choose all positive pairs and for each image, choose the
    'hardest' $x$ negative pairs: the pairs giving the highest loss.
    This choice of the hardest pairs is repeated before each epoch
    during training.
\end{enumerate}

The following CNNs were tested:
\begin{enumerate}
    \item AlexNet (only last convolutional layer is fine-tuned)
    \item ResNet-152 (last 3 blocks are fine-tuned)
\end{enumerate}

\subsection{Features of a fine-tuned Siamese CNN using a triplet loss}
This resembles the architecture of Gordo et al~\cite{gordo_deep_2016}
the most: only region pooling of the convolutional features is omitted.

The loss is defined for an anchor image, a reference positive image and
a reference negative image and their feature vectors $x_a$, $x_p$, $x_n$
respectively:

\begin{equation}
\mathcal{L} = \frac{1}{2}
\max(0, \| x_a - x_p \|_2^2 - \| x_a - x_n \|_2^2 + margin)
\end{equation}

For normalized feature vectors $x_1$, $x_2$, the distance $\| x_1 - x_2\|$
between the vectors is closely related to the cosine similarity:
$\| x_1 - x_2 \|_2^2 = 2-2x_1x_2 = 2-2cos(x_1,x_2)$.
So the loss could be written as
$\max(0, x_ax_n - x_ax_p + margin^{\star})$ with
$margin^{\star} = \frac{1}{2} margin$. Both versions of the loss
are implemented, but we chose the margin $margin^{\star}$ as basis
since in our case, feature vectors are always normalized.

\subsubsection{Motivation}
The motivation behind using the triplet loss is laid out in the paper
by Weinberger et al~\cite{weinberger_distance_2006}, who first introduced
this loss, as well as the paper by Schroff et al~\cite{schroff_facenet:_2015}.
Essentially, this loss is more robust w.r.t. noise in the data, as
we cannot always perfectly project all couples of images of the same
instance onto the same point in space, when we have noisy data.

\subsubsection{Specifics}
All images are resized to $227 \times 227$, using a nearest neighbor
re-sampling filter. The margin in the loss is set to 0.

For fine-tuning the Siamese CNN, we use the following settings:
\begin{enumerate}
    \item Data augmentation: TODO
    \item Learning rate: TODO
    \item TODO
\end{enumerate}

The following choices of the triplets of images for training were tested:
\begin{enumerate}
    \item Choose all positive pairs and randomly choose a negative pair
    for each positive pair to form a triplet.
    \item Choose all positive pairs and for each image, choose the
    'hardest' negative pair to form a triplet.
    \item Choose all positive pairs and for each image, choose 'semi-hard'
    negatives early on in training, then choose only the 'hardest'
    negatives. This is motivated by Schroff et al~\cite{schroff_facenet:_2015}.
    \item Choose the easiest positives along with the hardest negatives.
    This is motivated by the strategy of Gordo et al~\cite{gordo_deep_2016}.
    TODO add more precise strategy (see mail)
\end{enumerate}

The following CNNs were tested:
\begin{enumerate}
    \item AlexNet (only last convolutional layer is fine-tuned)
    \item ResNet-152 (last 3 blocks are fine-tuned)
\end{enumerate}

\section{Results}
Current results for each approach will be added here.

\section{Current work}
Currently, we need to figure out why there are discrepancies between
our results and CORIA results... TODO (add visualizations to see what goes wrong etc)

\section{Future work}
Propose new approach based on the problems found from analyzing current
systems/approaches.
Motivation: current approaches seem to give far from optimal results

Possibly create new dataset (based on images of museum art pieces)
Motivation: CLICIDE dataset is particularly suited for ORB. GaRoFou
dataset is very small. This makes it difficult, but it might not be
a good indicator of performance in general (the smaller the dataset,
the higher the risk of over-fitting).
A better dataset would be one with more images
(and proportionally more instances) and a better harmonization of
query vs reference images: reference images should be clean and from
all angles of the object, query images should not be clean, blurred,
only small parts of the image contains object to find etc.

TODO

\bibliographystyle{ieeetr}
\bibliography{siamese}

\newpage
\appendix
\section{Implementation details and specifics}
PyTorch
specific issues: large images/large batch sizes/large nets -> lots of memory

\end{document}
