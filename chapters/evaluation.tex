% !TeX root = ../full_report.tex

\chapter{Evaluation}\label{sec:evaluation}
\section{Datasets}\label{sec:datasets}
The proposed approaches as well as several base-lines are evaluated
on two datasets: the CLICIDE and GaRoFou datasets. These datasets are
described in detail by Portaz et al.~\cite{portaz_construction_2017}.
Both datasets are typical of instance search datasets
in museums or touristic sites: the objects represented by their images
are paintings for one and glass cabinets containing sculptures and artifacts
for the other dataset. Both datasets contain a small number of images
per instance and a small number of images in total.
Table~\ref{tab:datasets} lists the content and statistics of the datasets
in detail. In particular, this table shows that the total number of images
is quite small (order of magnitude of 1000), the number of instances
is large both in the dataset as well as the test sets (order of magnitude
of 100), and the median number of images per instance is small
(2 for GaRoFou, 6 for CLICIDE). This sets both datasets clearly apart
from the most common datasets used in image retrieval, such as
Oxford~\cite{philbin_object_2007}, Paris~\cite{philbin_lost_2008}
as well as Holidays~\cite{jegou_hamming_2008}.

\begin{table}
\begin{tabular}{|l|l|l|}
\hline & \emph{CLICIDE} & \emph{GaRoFou}\\
\hline \emph{Type of objects} & Paintings & Glass cabinets
containing various objects\\
\hline \emph{\#Instances} & 464 & 311\\
\hline \emph{\#Reference images} & 3245 & 1068\\
\hline \emph{\#Query images (\#instances)} & 165 (134) & 184 (166)\\
\hline \emph{Median \#images per instance} & 6 & 2\\
\hline \emph{Min \#images per instance} & 1 & 1\\
\hline \emph{Max \#images per instance} & 22 & 45\\
\hline \emph{Total reference images (\#instances)} & 3452 (473) & 1068 (311)\\
\hline \emph{Total query images (\#instances)} & 177 (143) & 184 (166)\\
\hline
\end{tabular}
\caption{Details of the datasets used for evaluation\label{tab:datasets}}
\end{table}

\subsection{CLICIDE}
The CLICIDE dataset contains photographs taken in the Grenoble Museum of Art.
The objects represented in the photographs are paintings and still images,
exclusively. The full dataset contains 207 images with no meaningful content,
mostly showing walls. These images are labeled as such and have been filtered
from the dataset in our evaluation. This explains the difference between
total number of images and number of images in Table~\ref{tab:datasets}.
Furthermore, there are 12 query images which contain objects not
present in the reference images. These have been filtered out as well.

The CLICIDE dataset is characteristic because the different images
for each instance usually consist of at least one global view of the
painting and multiple other images representing sub-regions of the
same painting.

\subsection{GaRoFou}
The GaRoFou dataset contains high-resolution photographs taken in the
Museun of Fourvi√®re in Lyon. The objects represented in the photographs
are a cultural heritage collection: sculptures, steles and small artifacts
grouped together in glass cabinets.

The full GaRoFou dataset is larger than the part used here: along with
the dataset of images, it contains a dataset of videos as well as images
extracted from these videos. The video collection has not been used as
part of our research.

\section{Metrics}
There are several commonly used metrics in classification and information
retrieval. The following sections define these metrics and their use
in our research.

\subsection{Precision}\label{sec:precision}
\begin{equation}\label{eq:preccls}
P = \frac{TP}{PRED}
\end{equation}

\begin{equation}\label{eq:precir}
P_q = \frac{RR_q}{RET}
\end{equation}

\begin{equation}\label{eq:preck}
P_q@k = \frac{RR^k_q}{k}
\end{equation}

\begin{equation}\label{eq:prec1}
MP@1 = \frac{1}{Q} \sum_q P_q@1
\end{equation}

In classification, precision is a measure of how many correct predictions
were made out of all predictions. Equation~\ref{eq:preccls} shows this
relationship: $TP$ are the true positives, the correct predictions,
and $PRED$ is the number of all predictions, including incorrect ones.
$PRED$ is usually equal to the size of the testing dataset in classification.

In information retrieval, similarly,
precision and more generally precision$@k$ is a measure of how many
relevant documents were retrieved out of all retrieved documents.
However, the metric is evaluated for each query, instead of the whole
testing dataset.
Equations~\ref{eq:precir}~and~\ref{eq:preck} illustrate this: $P_q$ is
the precision for query $q$, $RR_q$ represents the number of retrieved
and relevant documents for query $q$.
$RET$ represents the total number of retrieved documents. Finally, $RR^k_q$
represents the number of retrieved, relevant documents out of the $k$ first
retrieved documents for query $q$.

Thus, precision$@k$ simply uses a cut-off at $k$: it measures how many
relevant documents there are in the first $k$ retrieved documents.

In instance search, only the first retrieved document is important:
If the first result is correct, we correctly identify the instance.
Hence, we are mostly interested in the precision@1 metric here.
In order to generalize this metric to all queries, the mean precision@1
is used, as defined in Equation~\ref{eq:prec1}. In this equation,
$Q$ represents the number of queries.

\subsection{Mean average precision}
\begin{equation}\label{eq:recall}
R_q = \frac{RR_q}{REL_q}
\end{equation}

\begin{equation}\label{eq:ap}
AP_q = \frac{1}{REL_q} \sum_{k=1}^{RET} rel(RET_k)P@k
\end{equation}

\begin{equation}\label{eq:map}
MAP = \frac{1}{Q} \sum_q AP_q
\end{equation}

In information retrieval, average precision is commonly used, since it
allows to consider the order in which documents are returned by a system.
Average precision represents the area under the precision-recall curve
for a given query $q$. Recall for query $q$ is defined as in
Equation~\ref{eq:recall} as the fraction of retrieved, relevant documents
for query $q$ as compared to all relevant documents to query $q$.

Equation~\ref{eq:ap} defines average precision for a given query $q$:
it can be expressed as a finite sum
over the precision@k values at every possible cut-off $k$, multiplied by an
indicator $rel(RET_k)$, which is 1 if the $k$-th retrieved document is
relevant. This sum is then normalized by the total number of relevant
documents for query $q$ $REL_q$.
Note that in the literature, different definitions of average precision can
be found. We chose to follow a common definition of this metric, which
is shared by the Oxford~\cite{philbin_object_2007} and the
Paris~\cite{philbin_lost_2008} datasets.

Finally, mean average precision $MAP$ represents the mean of all average
precision values over $Q$ queries, as defined in Equation~\ref{eq:map}.

In instance search, mean average precision is not as important as in image retrieval,
since we only care about the first ranked result. Nevertheless, it can be an interesting
metric to allow better comparison between different systems, so we compute
the mean average precision for all results along with the mean precision@1.

\section{Evaluation methodology}\label{sec:evalmethod}
Unless explicitly noted, an evaluation of the system is carried out as
follows:

All reference images are encoded with a descriptor extracted from the
network to be tested. The query image is encoded the same way.
All descriptors are normalized and the descriptor of the query image
is compared to all reference images by computing the dot product between
it and all descriptors of the reference images. This can be performed
by a single vector-matrix multiplication. The resulting values describe
the cosine similarity between the descriptor of the query image
and the descriptors of reference images.

This is because the cosine similarity between two normalized vectors is
equal to the dot product between the two vectors, from the definition
of the cosine similarity. Furthermore, Equation~\ref{eq:sqdistcosdot} shows
that the dot product is closely related to the squared
euclidean distance between two normalized vectors.
This means that a high cosine similarity, high value of the dot product
and low squared euclidean distance between two descriptors have the
same meaning in the context of this research.

Hence, after a query image is compared to all reference images through
a vector-matrix multiplication, the index of the highest value in the
resulting vector indicates the most similar reference image.

\section{Baselines}\label{sec:baselines}
Table~\ref{tab:baselines} shows the baselines set for the two evaluation
datasets, as described in Section~\ref{sec:datasets}. The metric used
to compare systems is mean precision@1, as described in
Section~\ref{sec:precision}, and the evaluation is carried out as
described in Section~\ref{sec:evalmethod}.

\begin{table}
\begin{tabular}{|l|c|c|}
\hline & \emph{CLICIDE} & \emph{GaRoFou}\\
\hline \emph{SIFT} & 70.08 & 78.82\\
\hline \emph{AlexNet IN} & 72.73 & 85.87\\
\hline \emph{ResNet-152 IN} & 72.12 & 85.33\\
\hline \emph{Gordo et al.}~\cite{gordo_deep_2016} (ResNet-50) & 90.30 & \textbf{95.65}\\
\hline \emph{Gordo et al.}~\cite{gordo_deep_2016} (ResNet-50, multi-res)
& \textbf{92.73} & \textbf{95.65}\\
\hline
\end{tabular}
\caption{Evaluation base-lines set for the CLICIDE and GaRoFou datasets.
The results are expressed in percentage points of
mean precision@1\label{tab:baselines}}
\end{table}

The descriptors used for comparing images are extracted using different
methods.
The SIFT method has been previously evaluated by
Portaz et al.~\cite{portaz_construction_2017}, who have detailed this
evaluation.

For the \emph{AlexNet IN} and \emph{ResNet-152 IN} methods,
an AlexNet and ResNet-152 architecture is pre-trained on the ImageNet dataset.
Then, the descriptor is simply extracted by passing an image through the
convolutional layers of the
network and normalizing the output. For the ResNet architecture, the final
average pooling is performed as well to produce a descriptor of similar
dimensions to the others. This means the descriptor has dimension 2048
for the ResNet architecture and 9216 for the AlexNet architecture.

For all networks presented here which require images of a specific size,
all images are simply pre-processed by resizing them to the expected
size. Since the networks are based on networks built for the ImageNet
challenge
~\cite{russakovsky_imagenet_2015,he_deep_2016,krizhevsky_imagenet_2012},
the expected size is $224 \times 224$. The results of pre-trained
networks on ImageNet (\emph{AlexNet IN} and \emph{ResNet-152 IN}),
fine-tuned networks on classification (Section~\ref{sec:finetuning}) and
simplified Siamese networks (Section~\ref{sec:simplifiedsiam}) all use
images resized to $224 \times 224$.

The method by Gordo et al.~\cite{gordo_deep_2016}
is based on the pre-trained network published publicly, based on a ResNet-50
architecture. For this network, images are only pre-processed to have a
common size of 800 pixels on the smaller side of the image,
as the network works with region proposals and thus accepts
any scale of image. This network outputs a descriptor with 2048 dimensions.
Finally, the multi-resolution method uses input images with different
sizes of the smaller side (550, 800 and 1050 pixels).

\section{Performance}
The performance evaluation is carried out on a typical system used for
training CNNs.
The components approximately represent the highest standard available for a
single-GPU server architecture in 2016.

The server contains the following components:
\begin{enumerate}
    \item CPU: Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered}
    E5-2623 v4, 16 cores, 10MB cache, 2.60GHz
    \item Memory: 144 GB, DDR4 % TODO mem speed
    \item Graphics card: NVIDIA\textsuperscript{\textregistered} Titan X, Pascal architecture,
    12 GB memory (two of such graphics cards were available but only one
    was used in the performance evaluation at a time)
\end{enumerate}

Performance is measured in time in milliseconds or hours required to
execute the task, as well as GPU memory required.
If multiple sub-tasks are performed for
a task, the reported performance is simply the accumulation of time and
the maximum of required GPU memory of the sub-tasks.
\subsection{Training}
The training performance is evaluated using the server as described above
and the CLICIDE dataset as described in Section~\ref{sec:datasets},
containing 3245 images.

The training performance was measured only for tasks specific to the dataset,
as other tasks can be performed once upfront for any dataset and parameter
weights can be transferred in a negligible amount of time.
In particular, this means
that pre-training a network on a dataset not specific to our task is
counted as using no time or memory.

\subsection{Testing}
The testing performance is evaluated using the server as described above and
the validation dataset of CLICIDE, as described in Section~\ref{sec:datasets}.
This dataset contains 165 images. However, the performance is reported
as the average performance for a single image, as this is the most common
use-case when applying the instance search system.
