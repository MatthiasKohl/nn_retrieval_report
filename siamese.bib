
@inproceedings{radenovic_cnn_2016,
	title = {{CNN} {Image} {Retrieval} {Learns} from {BoW}: {Unsupervised} {Fine}-{Tuning} with {Hard} {Examples}},
	shorttitle = {{CNN} {Image} {Retrieval} {Learns} from {BoW}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-46448-0_1},
	doi = {10.1007/978-3-319-46448-0_1},
	abstract = {Convolutional Neural Networks (CNNs) achieve state-of-the-art performance in many computer vision tasks. However, this achievement is preceded by extreme manual annotation in order to perform either training from scratch or fine-tuning for the target task. In this work, we propose to fine-tune CNN for image retrieval from a large collection of unordered images in a fully automated manner. We employ state-of-the-art retrieval and Structure-from-Motion (SfM) methods to obtain 3D models, which are used to guide the selection of the training data for CNN fine-tuning. We show that both hard positive and hard negative examples enhance the final performance in particular object retrieval with compact codes.},
	language = {en},
	urldate = {2017-03-10},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer, Cham},
	author = {Radenović, Filip and Tolias, Giorgos and Chum, Ondřej},
	month = oct,
	year = {2016},
	pages = {3--20},
	file = {1604.02426.pdf:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/ZHS4MWC9/1604.02426.pdf:application/pdf;Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/6N4J8SNI/978-3-319-46448-0_1.html:text/html}
}

@inproceedings{gordo_deep_2016,
	title = {Deep {Image} {Retrieval}: {Learning} {Global} {Representations} for {Image} {Search}},
	shorttitle = {Deep {Image} {Retrieval}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-46466-4_15},
	doi = {10.1007/978-3-319-46466-4_15},
	abstract = {We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we leverage a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we use a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. Additional material is available at www.xrce.xerox.com/Deep-Image-Retrieval.},
	language = {en},
	urldate = {2017-03-10},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer, Cham},
	author = {Gordo, Albert and Almazán, Jon and Revaud, Jerome and Larlus, Diane},
	month = oct,
	year = {2016},
	pages = {241--257},
	file = {1604.01325.pdf:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/XM3S297Z/1604.01325.pdf:application/pdf;Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/PNN96KZJ/978-3-319-46466-4_15.html:text/html}
}

@inproceedings{sun_deep_2014,
	title = {Deep learning face representation by joint identification-verification},
	url = {http://papers.nips.cc/paper/5416-analog-memories-in-a-balanced-rate-based-network-of-e-i-neurons.pdf},
	urldate = {2017-03-10},
	booktitle = {Advances in neural information processing systems},
	author = {Sun, Yi and Chen, Yuheng and Wang, Xiaogang and Tang, Xiaoou},
	year = {2014},
	pages = {1988--1996},
	file = {[PDF] nips.cc:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/X8I89QBK/Sun et al. - 2014 - Deep learning face representation by joint identif.pdf:application/pdf}
}

@article{weinberger_distance_2006,
	title = {Distance metric learning for large margin nearest neighbor classification},
	volume = {18},
	url = {https://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf},
	urldate = {2017-03-10},
	journal = {Advances in neural information processing systems},
	author = {Weinberger, Kilian Q. and Blitzer, John and Saul, Lawrence},
	year = {2006},
	pages = {1473},
	file = {[PDF] nips.cc:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/5SGUSRK8/Weinberger et al. - 2006 - Distance metric learning for large margin nearest .pdf:application/pdf}
}

@inproceedings{schroff_facenet:_2015,
	title = {Facenet: {A} unified embedding for face recognition and clustering},
	shorttitle = {Facenet},
	url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
	urldate = {2017-03-10},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
	year = {2015},
	pages = {815--823},
	file = {[PDF] cv-foundation.org:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/6WC82GH2/Schroff et al. - 2015 - Facenet A unified embedding for face recognition .pdf:application/pdf;Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/HCGSSDDR/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html:text/html}
}

@inproceedings{chopra_learning_2005,
	title = {Learning a similarity metric discriminatively, with application to face verification},
	volume = {1},
	doi = {10.1109/CVPR.2005.202},
	abstract = {We present a method for training a similarity metric from data. The method can be used for recognition or verification applications where the number of categories is very large and not known during training, and where the number of training samples for a single category is very small. The idea is to learn a function that maps input patterns into a target space such that the L1 norm in the target space approximates the "semantic" distance in the input space. The method is applied to a face verification task. The learning process minimizes a discriminative loss function that drives the similarity metric to be small for pairs of faces from the same person, and large for pairs from different persons. The mapping from raw to the target space is a convolutional network whose architecture is designed for robustness to geometric distortions. The system is tested on the Purdue/AR face database which has a very high degree of variability in the pose, lighting, expression, position, and artificial occlusions such as dark glasses and obscuring scarves.},
	booktitle = {2005 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'05)},
	author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
	month = jun,
	year = {2005},
	keywords = {Artificial neural networks, Character generation, discriminative loss function, Drives, face recognition, face verification, geometric distortion, Glass, L1 norm, learning (artificial intelligence), Robustness, semantic distance approximation, similarity metric learning, Spatial databases, Support vector machine classification, Support vector machines, System testing},
	pages = {539--546 vol. 1},
	file = {cvpr05.pdf:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/2KGM36QV/cvpr05.pdf:application/pdf;IEEE Xplore Abstract Record:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/T2FP3MD6/1467314.html:text/html}
}

@article{tolias_particular_2015,
	title = {Particular object retrieval with integral max-pooling of {CNN} activations},
	url = {http://arxiv.org/abs/1511.05879},
	abstract = {Recently, image representation built upon Convolutional Neural Network (CNN) has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image re-ranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets.},
	urldate = {2017-03-10},
	journal = {arXiv:1511.05879 [cs]},
	author = {Tolias, Giorgos and Sicre, Ronan and Jégou, Hervé},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.05879},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.05879 PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/I24943MU/Tolias et al. - 2015 - Particular object retrieval with integral max-pool.pdf:application/pdf;arXiv.org Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/QZKSFCDC/1511.html:text/html}
}

@article{mishkin_systematic_2016,
	title = {Systematic evaluation of {CNN} advances on the {ImageNet}},
	url = {http://arxiv.org/abs/1606.02228},
	abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
	urldate = {2017-03-10},
	journal = {arXiv:1606.02228 [cs]},
	author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.02228},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.02228 PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/QNHW29C9/Mishkin et al. - 2016 - Systematic evaluation of CNN advances on the Image.pdf:application/pdf;arXiv.org Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/4DJFXCZ7/1606.html:text/html}
}

@article{iandola_squeezenet:_2016,
	title = {{SqueezeNet}: {AlexNet}-level accuracy with 50x fewer parameters and {\textless}0.5MB model size},
	shorttitle = {{SqueezeNet}},
	url = {http://arxiv.org/abs/1602.07360},
	abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
	urldate = {2017-03-10},
	journal = {arXiv:1602.07360 [cs]},
	author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.07360},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1602.07360 PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/M53HCSF8/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer .pdf:application/pdf;arXiv.org Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/XDPS9JNM/1602.html:text/html}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	urldate = {2017-03-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {1097--1105},
	file = {NIPS Full Text PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/WCGZEHGG/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf;NIPS Snapshort:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/3TFPHGWQ/4824-imagenet-classification-with-deep-convolutional-neural-networks.html:text/html}
}

@article{simonyan_very_2014,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2017-03-10},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1409.1556 PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/URH4NTMM/Simonyan and Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/3FJTQNN5/1409.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2017-03-10},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1512.03385 PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/75TWFUZ6/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/58QU6D8S/1512.html:text/html}
}

@article{szegedy_inception-v4_2016,
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	url = {http://arxiv.org/abs/1602.07261},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
	urldate = {2017-03-10},
	journal = {arXiv:1602.07261 [cs]},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.07261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1602.07261 PDF:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/9F5UTUNV/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of R.pdf:application/pdf;arXiv.org Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/69BMMSHX/1602.html:text/html}
}

@article{mikulik_learning_2013,
	title = {Learning {Vocabularies} over a {Fine} {Quantization}},
	volume = {103},
	issn = {0920-5691, 1573-1405},
	url = {https://link.springer.com/article/10.1007/s11263-012-0600-1},
	doi = {10.1007/s11263-012-0600-1},
	abstract = {A novel similarity measure for bag-of-words type large scale image retrieval is presented. The similarity function is learned in an unsupervised manner, requires no extra space over the standard bag-of-words method and is more discriminative than both L2-based soft assignment and Hamming embedding. The novel similarity function achieves mean average precision that is superior to any result published in the literature on the standard Oxford 5k, Oxford 105k and Paris datasets/protocols. We study the effect of a fine quantization and very large vocabularies (up to 64 million words) and show that the performance of specific object retrieval increases with the size of the vocabulary. This observation is in contradiction with previously published results. We further demonstrate that the large vocabularies increase the speed of the tf-idf scoring step.},
	language = {en},
	number = {1},
	urldate = {2017-03-10},
	journal = {International Journal of Computer Vision},
	author = {Mikulik, Andrej and Perdoch, Michal and Chum, Ondřej and Matas, Jiří},
	month = may,
	year = {2013},
	pages = {163--175},
	file = {download.pdf:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/IXU2C5RC/download.pdf:application/pdf;Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/5C9AQAFK/s11263-012-0600-1.html:text/html}
}

@inproceedings{philbin_object_2007,
	title = {Object retrieval with large vocabularies and fast spatial matching},
	url = {http://ieeexplore.ieee.org/abstract/document/4270197/},
	urldate = {2017-03-10},
	booktitle = {Computer {Vision} and {Pattern} {Recognition}, 2007. {CVPR}'07. {IEEE} {Conference} on},
	publisher = {IEEE},
	author = {Philbin, James and Chum, Ondrej and Isard, Michael and Sivic, Josef and Zisserman, Andrew},
	year = {2007},
	pages = {1--8},
	file = {[PDF] ox.ac.uk:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/VN8NX7SX/Philbin et al. - 2007 - Object retrieval with large vocabularies and fast .pdf:application/pdf;Snapshot:/home/kohlm/.mozilla/firefox/n3dzy4ns.default/zotero/storage/WNAN9FGM/4270197.html:text/html}
}