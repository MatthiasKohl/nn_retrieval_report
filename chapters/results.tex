% !TeX root = ../full_report.tex

\chapter{Results}
In this section, we present the experimental results obtained in our
research.

\section{Performance}\label{sec:perfresults}
Table~\ref{tab:perftrain} shows the performance of various approaches
when training on the CLICIDE dataset. We assume that pre-trained
networks have 0 cost. This table shows that fine-tuning a network
on classification on the new dataset is quite fast and has a low memory
footprint. This is interesting since fine-tuning already achieves better
results than many standard approaches, as can be seen in
Section~\ref{sec:evalresults}.
We can also see that training our proposed architecture is much
more costly, although the absolute cost is not excessive:
around 48h (TODO) of training time in total for the very deep ResNet-152.

Table~\ref{tab:perftest} shows the performance of these approaches
when evaluating them on a single image. We can see that all approaches
have approximately the same performance, with % TODO.

\begin{table}
\begin{tabular}{|l|c|c|}
\hline & \multicolumn{2}{c|}{\emph{CLICIDE training}}\\
\hline & Time (h) & GPU memory (MiB)\\
\hline \emph{AlexNet IN} & 0 & 0\\
\hline \emph{ResNet-152 IN} & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016}} & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (multi-resolution)}
& 0 & 0\\
\hline \emph{AlexNet FT} & 1.429 & 1706\\
\hline \emph{ResNet-152 FT} & 1.621 & 6910\\
\hline \emph{Proposed AlexNet} & 38.24 & 1956\\
\hline \emph{Proposed ResNet-152} & 71.92 & 10494\\
\hline
\end{tabular}
\caption{Performance of different approaches in terms of time in seconds and
maximal GPU memory usage in MB when training on the CLICIDE dataset
\label{tab:perftrain}}
\end{table}

\begin{table}
\begin{tabular}{|l|c|c|}
\hline & \multicolumn{2}{c|}{\emph{CLICIDE test per image}}\\
\hline & Time (ms) & GPU memory (MiB)\\
\hline \emph{AlexNet IN} & 0 & 0\\ % TODO !
\hline \emph{ResNet-152 IN} & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016}} & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (multi-resolution)}
& 0 & 0\\
\hline \emph{AlexNet FT} & 4.6 & 1012\\
\hline \emph{ResNet-152 FT} & 26.1 & 944\\
\hline \emph{Proposed AlexNet} & 12.3 & 1116\\
\hline \emph{Proposed ResNet-152} & 45.2 & 2212\\
\hline
\end{tabular}
\caption{Performance of different approaches in terms of time in seconds and
maximal GPU memory usage in MB when testing on a single image
of the CLICIDE dataset\label{tab:perftest}}
\end{table}

Note that the performance reported for testing is not relevant in absolute numbers,
since it was established on a high-end server, usually unavailable when testing
a system. However, the results are relevant in relative terms when comparing
different systems.

% TODO talk about results, what we see etc
% TODO make test on empty decore0 server with one Titan X (possibly extrapolate from 5 epochs or so)

% TODO make test on empty decore0 server with one Titan X
% TODO talk about some performance metrics (how much time to train, to evaluate, depending on network. what hardware was used)

\section{Evaluation results}\label{sec:evalresults}
In this section, we present the results obtained on the presented datasets
using our network as described in Section~\ref{sec:proposed}, as well
as various other approaches.

\begin{table}
\begin{tabular}{|l|c|c|c|c|}
\hline & \multicolumn{2}{c|}{\emph{Mean Precision@1}} &
\multicolumn{2}{c|}{\emph{Mean Average Precision}}\\
\hline & \emph{CLICIDE} & \emph{GaRoFou} & \emph{CLICIDE} & \emph{GaRoFou}\\
\hline \emph{SIFT} & 70.08 & 78.82 & N/A & N/A\\
\hline \emph{AlexNet IN} & 57.58 & 76.63 & 0 & 0\\
\hline \emph{ResNet-152 IN} & 64.24 & 75.54 & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016}}
& 90.30 & 95.65 & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (multi-resolution)}
& 92.73 & 95.65 & 0 & 0\\
\hline \emph{AlexNet FT} & 0 & 0 & 0 & 0\\ % TODO !
\hline \emph{ResNet-152 FT} & 0 & 0 & 0 & 0\\
\hline \emph{Proposed AlexNet} & 81.21 & 83.15 & 45.53 & 71.71\\ % TODO !
\hline \emph{Proposed ResNet-152} & 94.55 & 96.20 & 82.94 & 91.83\\
\hline \emph{Proposed AlexNet (IFA)} & 80.61 & 82.61 & 71.02 & 81.66\\ % TODO !
\hline \emph{Proposed ResNet-152 (IFA)} & 93.94 & 95.11 & 94.23 & 93.86\\
\hline
\end{tabular}
\caption{Evaluation results for the CLICIDE and GaRoFou datasets.
The results are expressed in percentage points of
Mean Precision@1 and mean average precision (only indicative)
\label{tab:results}}
\end{table}

Table~\ref{tab:results} gives an overview of the results obtained. First,
the baselines established in Section~\ref{sec:baselines} are shown
again, with the addition of the relevant results obtained by fine-tuning
a classification network (Section~\ref{sec:finetuning}) and our
proposed network (Section~\ref{sec:proposed}).
In addition to the mean precision@1, we show the mean average
precision obtained by the different approaches.

In Table~\ref{tab:results}, \emph{IFA} refers to the results obtained
using instance feature augmentation, as presented in Section~\ref{sec:ifa}.

% TODO  possibly remove this
The comparison of performance between different descriptors should
be based on the mean precision@1. However, the mean average
precision is indicative of how well the model
captures the pattern or characteristics which defines an instance or
sets it apart from other instances. In particular, it is possible to obtain
a high mean precision@1 while achieving a low mean average precision
if the descriptor describes visual similarity well enough. This is because
there is usually at least one reference image that is visually similar
to the query. However, in order to achieve a high mean average precision,
the descriptor needs to capture the specific characteristics of the
instance that set it apart from other instances. For these reasons,
we show the mean average precision obtained by the different
descriptors along with the mean precision@1.
