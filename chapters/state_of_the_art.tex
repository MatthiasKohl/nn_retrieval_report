% !TeX root = ../full_report.tex

\chapter{State of the art}
\section{SIFT and bag-of-words}
Until recently, the state of the art in image retrieval and matching
was based on the idea of
bag-of-words~\cite{philbin_object_2007}~\cite{mikulik_learning_2013}.
The idea behind the approach is to represent an image as a histogram,
or collection of frequencies, of visual words. The visual words should
be small, representative patches of the images in the dataset.
Usually, these visual words are obtained in multiple steps. First,
local features are extracted and encoded. The most common feature used
is SIFT~\cite{TODO}, a 128-dimensional vector representing
scale-invariant features. Then, the features are extracted for all
images and clustered into clusters of similar features.
The representative for each cluster is the center of the cluster,
i.e. the mean of all features falling into that cluster.
Each image can then be represented as a histogram of the occurrences
of these representative features.
This histogram forms the image descriptor, which has as many dimensions
as there are representative features and clusters.

Finally, for image classification, a classifier can be learned from the
descriptors of all images in the dataset. On the other hand, for image
retrieval or matching, the descriptors can be directly matched against
each other, based on some similarity measure.
In both cases, it can be useful to project the descriptors into a Hilbert
space using a different similarity measure than euclidean distance
in the original descriptor space.
For classification, this is done by employing an SVM classifier with a
non-linear kernel~\cite{TODO}. Among the most popular kernels are the
radial basis function~\cite{} and the chi-squared function~\cite{}.

Until recently, this approach has obtained state of the art results for
image retrieval tasks~\cite{mikulik_learning_2013}.

\section{Deep learning with CNNs}
Starting with the results of AlexNet for image classification in the 2012
ImageNet challenge~\cite{krizhevsky_imagenet_2012}~\cite{TODO},
image classification tasks have been dominated by CNNs, learned using
large amounts of data.

A general trend in image related tasks is to move to an end-to-end
approach, where the final objective is directly optimized using gradient
descent and the gradient is back-propagated to all previous parts of the
system. In contrast, the bag-of-words model requires a choice of
features (SIFT, ORB (TODO mention ORB before), \dots),
a choice of the method for clustering features (k-means),
a choice of the classifier (AdaBoost~\cite{}, SVM, \dots),
as well as a choice of the kernel if an SVM classifier is used.

Using a CNN, features are extracted at a low abstraction level by the
first convolutional layers, then higher level features are formed
by combining low level features from the previous layers. Finally,
high-level features are combined into a classifier by linear layers.
The advantage of this approach is that the features are learned at all
abstraction levels. Furthermore, the modularity of the approach allows
us to easily transfer the lower level features learned from a large dataset
to a smaller dataset, where there may not be enough data to efficiently
learn lower level features.

After AlexNet, many improvements to the CNN architecture have been
suggested. First, the VGG architecture uses smaller convolutional kernels
and thus more layers to reduce the number of parameters and increase
the non-linearity of the network~\cite{simonyan_very_2014}. Second,
even deeper networks were suggested. However, a very deep network
is hard to train because of the vanishing gradient problem~\cite{}.
This problem is overcome in the ResNet~\cite{he_deep_2015} and
Inception~\cite{szegedy_inception-v4_2016} architectures by introducing
skip connections. A skip connection means that, for a block of two or
three layers, the output of the previous layer is added to the output
of the block. In the Inception architecture, skip connections of different
lengths are combined. Finally, the DenseNet~\cite{} architecture takes
the skip connection idea one step further: each layer is dependent on
the output of all $n$ previous layers.

\section{Image retrieval using CNNs}
For image retrieval, the current state of the art set by
Gordo et al~\cite{gordo_deep_2016} is based on such an
end-to-end approach. The goal is to learn a global descriptor for images
that is well suited for comparing images.
This is achieved by extracting the convolutional features of a pre-trained
CNN. Then, a Region Proposal Network (RPN)~\cite{TODO faster rcnn} is
used to extract the regions of interest. For each region of interest,
a shifting and linear layer are used to reduce the dimensionality of
the descriptor. The final descriptor is simply a normalized sum of the
region-wise descriptors. This network can be learned end-to-end and
the obtained descriptor achieves state of the art results, which can
be even further improved by using query expansion and database side
feature augmentation (TODO either remove or mention before).

