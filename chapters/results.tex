% !TeX root = ../full_report.tex

\chapter{Results}\label{chap:results}
In this section, we present the experimental results obtained in our
research.

\section{Evaluation results}\label{sec:evalresults}
In this section, we present the results obtained on the presented datasets
using our network as described in Section~\ref{sec:proposed}, as well
as various other approaches.

\begin{table}
\centering
\begin{tabular}{|l|c|c||c|c|}
\hline & \multicolumn{2}{c||}{\emph{Mean Precision@1}} &
\multicolumn{2}{c|}{\emph{Mean Average Precision}}\\
\hline & \emph{CLICIDE} & \emph{GaRoFou} & \emph{CLICIDE} & \emph{GaRoFou}\\
\hline \emph{SIFT} & 70.08 & 78.82 & N/A & N/A\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (ResNet-50)}
& 90.30 & 95.65 & 65.49 & 88.43\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (ResNet-50, multi-res)}
& 92.73 & 95.65 & 70.36 & 89.32\\
\hline \emph{AlexNet IN} & 72.73 & 85.87 & 32.71 & 66.11\\
\hline \emph{ResNet-152 IN} & 72.12 & 85.33 & 40.99 & 70.15\\
\hline
\hline \emph{AlexNet FT} & 78.18 & 90.76 & 38.51 & 72.92\\
\hline \emph{AlexNet SS} & 75.76 & 90.20 & 36.20 & 77.73\\
\hline \emph{Proposed AlexNet} & 81.21 & 83.15 & 45.53 & 71.71\\
\hline \emph{Proposed AlexNet (IFA)} & 80.61 & 82.61 & 71.02 & 81.66\\
\hline \emph{ResNet-152 FT} & 79.39 & 94.57 & 75.11 & 93.44\\
\hline \emph{ResNet-152 SS} & 85.45 & 95.11 & 83.00 & 91.90\\
\hline \emph{Proposed ResNet-152} & \textbf{94.55} & \textbf{96.20}
& 82.94 & 91.83\\
\hline \emph{Proposed ResNet-152 (IFA)} & 93.94 & 95.11
& \textbf{94.23} & \textbf{93.86}\\
\hline
\end{tabular}
\caption{Evaluation results for the CLICIDE and GaRoFou datasets.
The results are expressed in percentage points of
mean precision@1 and mean average precision (only indicative)
\label{tab:results}}
\end{table}

Table~\ref{tab:results} gives an overview of the results obtained. First,
the baselines established in Section~\ref{sec:baselines} are shown
again. Additionally, we show the relevant results obtained by fine-tuning
a classification network, abbreviated by FT in the table. This method
is described in Section~\ref{sec:finetuning}. We then show the results
obtained by a simplified Siamese architecture, as described in Section
~\ref{sec:simplifiedsiam}, abbreviated SS. Finally, we show the results
obtained by the proposed network as described in Section~\ref{sec:proposed}.
In addition to the mean precision@1, we show the mean average
precision obtained by the different approaches.

In Table~\ref{tab:results}, all descriptors have dimension 2048, except
for the AlexNet IN and AlexNet FT methods. As described in
Section~\ref{sec:baselines}, these descriptors have dimension 9216,
since they consist of the convolutional features of an
AlexNet architecture.

In Table~\ref{tab:results}, \emph{IFA} refers to the results obtained
using instance feature augmentation, as presented in Section~\ref{sec:ifa}.

\section{Performance}\label{sec:perfresults}

\begin{table}
\centering
\begin{tabular}{|l|c|c|}
\hline & \multicolumn{2}{c|}{\emph{CLICIDE training}}\\
\hline & Time (h) & GPU memory (MiB)\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (ResNet-50)} & 0 & 0\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (ResNet-50, multi-res)}
& 0 & 0\\
\hline \emph{AlexNet IN} & 0 & 0\\
\hline \emph{ResNet-152 IN} & 0 & 0\\
\hline
\hline \emph{AlexNet FT} & 1.429 & 1706\\
\hline \emph{AlexNet SS} & 9.985 & 1720\\
\hline \emph{Proposed AlexNet} & 49.06 & 1956\\
\hline \emph{ResNet-152 FT} & 1.621 & 6910\\
\hline \emph{ResNet-152 SS} & 12.59 & 9332\\
\hline \emph{Proposed ResNet-152} & 86.62 & 10494\\
\hline
\end{tabular}
\caption{Performance of different approaches in terms of time in hours and
maximal GPU memory usage in MB when training on the CLICIDE dataset
\label{tab:perftrain}}
\end{table}

Table~\ref{tab:perftrain} shows the performance of various approaches
when training on the CLICIDE dataset. We assume that pre-trained
networks have 0 cost. This table shows that fine-tuning a network
on classification on the new dataset is quite fast and has a low memory
footprint. This is interesting since fine-tuning already achieves better
results than many standard approaches, as can be seen in
Section~\ref{sec:evalresults}.
We can also see that training our proposed architecture is much
more costly, although the absolute cost is not excessive:
around 86h of training time in total for the very deep ResNet-152
and 49h for AlexNet, using our larger dataset with over 3000 images.

\begin{table}
\centering
\begin{tabular}{|l|c|c|}
\hline & \multicolumn{2}{c|}{\emph{CLICIDE test per image}}\\
\hline & Time (ms) & GPU memory (MiB)\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (ResNet-50)} & 200 & N/A\\
\hline \emph{Gordo et al.~\cite{gordo_deep_2016} (ResNet-50, multi-res)}
& 922 & N/A\\
\hline \emph{AlexNet IN} & 1.14 & 850\\
\hline \emph{ResNet-152 IN} & 23.9 & 972\\
\hline
\hline \emph{AlexNet FT} & 4.6 & 1012\\
\hline \emph{AlexNet SS} & 6.27 & 800\\
\hline \emph{Proposed AlexNet} & 12.3 & 1116\\
\hline \emph{ResNet-152 FT} & 26.1 & 944\\
\hline \emph{ResNet-152 SS} & 27.6 & 1754\\
\hline \emph{Proposed ResNet-152} & 45.2 & 2212\\
\hline
\end{tabular}
\caption{Performance of different approaches in terms of average time in milliseconds and
maximal GPU memory usage in MB when extracting the descriptor of a single image
of the CLICIDE dataset\label{tab:perftest}}
\end{table}

Table~\ref{tab:perftest} shows the performance of the different networks
when evaluating on a single image. For the baseline based on SIFT, no performance measures were available.
For the Gordo et al.~\cite{gordo_deep_2016} baseline, we simply took the
measurements from their paper, performed on a slightly different system,
as we could only test a slower version of the model, which does not
evaluate the entire system on GPU, and is thus about 10 times slower than
what was reported in the paper.
For the results of the multi-resolution system, we scale the time
by a factor measured by this slower version between the multi-resolution
and the normal system.

We can see from Table~\ref{tab:perftest} that all networks
of the same architecture have approximately the same performance, with
the proposed architecture being 2-3 times slower than a network
fine-tuned on classification. However, compared to the best performing
network proposed by Gordo et al.~\cite{gordo_deep_2016}, the proposed AlexNet
is two orders of magnitude faster and the proposed ResNet152 is one
order of magnitude faster.

This means that the proposed AlexNet architecture can be evaluated in
real-time and the proposed ResNet152 almost in real-time on a suitable
server.
Note that this real-time performance may not be possible with the current
state of the art in mobile computing, but it may be an indication of
what will be possible in future mobile platforms.

The difference in performance can be mostly explained by the
differences in image
sizes used by the various systems. While a fine-tuned network uses
images of size $224 \times 224$, the proposed network uses images
with smaller side $448$ for ResNet-152 or $384$ for AlexNet and the
system by Gordo et al. uses images with smaller side $800$ and $550$,
$800$ and $1050$ for the multi-resolution version.

\section{Interpretation}
% TODO  possibly remove this
The comparison of performance between different descriptors should
be based on the mean precision@1. However, the mean average
precision is indicative of how well the model
captures the pattern or characteristics which defines an instance or
sets it apart from other instances. In particular, it is possible to obtain
a high mean precision@1 while achieving a low mean average precision
if the descriptor describes visual similarity well enough. This is because
there is usually at least one reference image that is visually similar
to the query. However, in order to achieve a high mean average precision,
the descriptor needs to capture the specific characteristics of the
instance that set it apart from other instances. For these reasons,
we show the mean average precision obtained by the different
descriptors along with the mean precision@1.

From the baselines presented in Section~\ref{sec:baselines}, we can make
two observations. First, even a simple global descriptor obtained from the
convolutional features of a CNN pre-trained on ImageNet performs better
than matching local SIFT descriptors on our datasets. Second, the ResNet-50
proposed by Gordo et al.~\cite{gordo_deep_2016} out-performs the descriptors
from pre-trained networks by far, even though it has never seen the images
from our datasets during training, either.

Table~\ref{tab:results} confirms these observations when taking into
account the mean average precision of the ResNet-50
and the convolutional features of networks pre-trained on ImageNet.
The difference is more than 10 points gained in mean
average precision even when comparing against the ResNet architecture.
This means that a ResNet fully optimized for image matching captures the
visual information much better than just the convolutional features
of a ResNet pre-trained on image classification.
This is expected, since that was one of the
goals of the approach proposed by Gordo et al.~\cite{gordo_deep_2016}.

Another observation we can make from Table~\ref{tab:results} is that
fine-tuning a network on the reference dataset consistently out-performs
a pre-trained network. Furthermore, the training cost of fine-tuning is
very low, as can be seen from Table~\ref{tab:perftrain}, and the testing
costs are equivalent, since the exact same architecture can be used.
This shows that transfer learning is very powerful for small datasets with
many classes. Indeed, networks with many parameters such as AlexNet and
ResNet could not have been trained on such small datasets with
uninitialized weights.

However, when comparing the classification fine-tuning method with the
simplified Siamese architecture (fine-tuning on image matching using a triplet loss),
it is not as clear which one performs better.
From the results, we can see that the classification fine-tuning has a better
performance for AlexNet while the triplet loss fine-tuning has a better
performance for ResNet-152. This is most likely due to two factors: the
hyper-parameters when training the Siamese AlexNet were not perfectly
suited, hence the convergence behavior is not as good as with the Siamese
ResNet. Furthermore, the AlexNet fine-tuned for classification has a much
larger descriptor of dimension 9216 versus the descriptor of dimension
2048 of the simplified Siamese architecture. This may explain that
the simplified Siamese architecture performs worse for AlexNet and
better for ResNet-152.

Finally, when comparing the proposed architecture with the previous ones,
it is clear that the proposed architecture out-performs all of them.
It achieves higher precision@1 as well as higher mean average precision,
especially when combined with the instance feature augmentation as described
in Section~\ref{sec:ifa}. The proposed ResNet-152 also out-performs the
network proposed by Gordo et al.~\cite{gordo_deep_2016}, based on a ResNet-50.
The comparison between these two networks is difficult though.
This is because on the on hand, our proposed
network is trained on the reference dataset used when comparing images,
giving it an unfair advantage. On the other hand, the ResNet-50
is trained on the much larger Landmarks dataset
~\cite{babenko_neural_2014}, giving it the advantage of data volume.
As shown in Sections~\ref{sec:tripletselection}~and~\ref{sec:roi},
the training methodology developed by Gordo et al.~\cite{gordo_end--end_2017}
is not applicable to a small, clean dataset, such as the ones used in our
evaluation.

A surprising result is the result given by instance feature augmentation as
described in Section~\ref{sec:ifa}. While the mean average precision is
greater than the mean average precision of the proposed network by a large
margin, the mean precision@1 is slightly lower. This may show that the
proposed network matched some outliers correctly, while the descriptors
produced by instance feature augmentation are more robust overall.
However, we do not have a more detailed explanation of this result as of now.

Note that the difference in mean average precision between the ResNet-50
by Gordo et al.~\cite{gordo_deep_2016} and the proposed ResNet-152 are
statistically significant, confirmed by a paired student's t-test with
$p$-value $p<0.001$. This is valid for both versions of the proposed
ResNet-152, including IFA or not, compared to both versions of the
ResNet-50, multi-resolution or not.
